{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Detect Duplicates Using Hashing\n",
    "\n",
    "#TODO: Write me.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from binascii import hexlify\n",
    "from datetime import datetime\n",
    "import hashlib\n",
    "from hmac import compare_digest\n",
    "import json\n",
    "import os\n",
    "\n",
    "import requests as r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hashing Introduction\n",
    "\n",
    "### What is a hash?\n",
    "\n",
    "A hash function is a deterministic mathematical function that takes an input of any length and content (e.g. letters, numbers, and symbols) and uses a formula to produce an output of a specific length.  \n",
    "\n",
    "### What can be hashed?\n",
    "\n",
    "A hash can be created using nearly any form of digitial content: a document, image, song, etc.\n",
    "\n",
    "\n",
    "reference: [What is hashing?](https://medium.com/tech-tales/what-is-hashing-6edba0ebfa67)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the hasher\n",
    "hasher = hashlib.sha256()\n",
    "\n",
    "# let's hash something\n",
    "hasher.update(b\"I am a fun hash!\")\n",
    "\n",
    "print(\"I am a fun hash!: \" + hasher.hexdigest())\n",
    "\n",
    "hasher.update(b\"To be or not to be\")\n",
    "\n",
    "print(\"I am a fun hash!  To be or not to be: \" + hasher.hexdigest())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MD5 (Messsage Digest) Hashing Examples\n",
    "\n",
    "hasher = hashlib.md5()\n",
    "\n",
    "shakespeare_sonnet = \"\"\"\n",
    "Shall I compare thee to a summer’s day?\n",
    "Thou art more lovely and more temperate:\n",
    "Rough winds do shake the darling buds of May,\n",
    "And summer’s lease hath all too short a date;\n",
    "Sometime too hot the eye of heaven shines,\n",
    "And often is his gold complexion dimm'd;\n",
    "And every fair from fair sometime declines,\n",
    "By chance or nature’s changing course untrimm'd;\n",
    "But thy eternal summer shall not fade,\n",
    "Nor lose possession of that fair thou ow’st;\n",
    "Nor shall death brag thou wander’st in his shade,\n",
    "When in eternal lines to time thou grow’st:\n",
    "So long as men can breathe or eyes can see,\n",
    "So long lives this, and this gives life to thee.\n",
    "\"\"\"\n",
    "\n",
    "hasher.update(shakespeare_sonnet.encode('utf-8'))\n",
    "\n",
    "print(hasher.hexdigest())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hashing an image\n",
    "\n",
    "paris_picture = 'https://lonelyplanetimages.imgix.net/mastheads/GettyImages-500759045_super.jpg?sharp=10&vib=20&w=1200'\n",
    "\n",
    "image_binary = r.get(paris_picture).content\n",
    "\n",
    "print('Image size: ' + str(len(image_binary)))\n",
    "\n",
    "print(b'some of the image content:' + image_binary[0:10])\n",
    "\n",
    "picture_hash = hashlib.md5(image_binary)\n",
    "\n",
    "print('Image hash: ' + picture_hash.hexdigest())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example:  How to detect duplicate dictionaries\n",
    "\n",
    "Suppose Alice sends Bob a series of JSON objects over the wire, but Alice is forgetful and sends Bob multiple JSON blobs containing the same data.  Bob doesn't want to have to read Alice's messages multiple times, so he needs a way to figure out if they're duplicate messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alices_messages = [\n",
    "    {'message': 'Hello, Bob!', 'timestamp': datetime.now()},\n",
    "    {'message': 'How are you doing, Bob?', 'timestamp': datetime.now()},\n",
    "    {'id': 12454432, 'message': 'I see you Bob!', 'timestamp': datetime.now()},\n",
    "    {'message': 'How are you doing, Bob?', 'timestamp': datetime.now()},\n",
    "]\n",
    "\n",
    "print(alices_messages)\n",
    "\n",
    "\n",
    "# bob gets the message batch\n",
    "\n",
    "for message in alices_messages:\n",
    "    encoded_message = message.get('message').encode('utf-8')\n",
    "    message_hash = hashlib.sha256(encoded_message).hexdigest()\n",
    "    print(f'{encoded_message}: {message_hash}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bob notices Alice is sending duplicate messages, and decides to create a dictionary to filter them.\n",
    "duplicate_timestamp = datetime.now()\n",
    "\n",
    "alices_messages = [\n",
    "    {'message': 'Hello, Bob!', 'timestamp': datetime.now(), 'weather': 'cloudy with a chance of meatballs'},\n",
    "    {'message': 'How are you doing, Bob?', 'timestamp': duplicate_timestamp},\n",
    "    {'id': 12454432, 'message': 'I see you Bob!', 'timestamp': datetime.now()},\n",
    "    {'message': 'How are you doing, Bob?', 'timestamp': duplicate_timestamp},\n",
    "]\n",
    "\n",
    "unique_messages = {}\n",
    "\n",
    "for message in alices_messages:\n",
    "    encoded_message = json.dumps(message, sort_keys=True, default=str)\n",
    "    print(f'encoded message: {encoded_message}')\n",
    "    message_hash = hashlib.sha256(encoded_message.encode('utf-8')).hexdigest()\n",
    "    print(f'hash: {message_hash}')\n",
    "    \n",
    "    # check to see if message exists, and if not add it to the docket\n",
    "    if message_hash not in unique_messages:\n",
    "        unique_messages.update({message_hash: message})\n",
    "    else:\n",
    "        print('Silly Alice, she sent a duplicate message!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(unique_messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cryptographically Secure Hashing\n",
    "\n",
    "### Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a key\n",
    "KEY_LENGTH = 30\n",
    "AUTH_SIZE = 16\n",
    "\n",
    "secret_key = hexlify(os.urandom(KEY_LENGTH))\n",
    "\n",
    "\n",
    "def sign(cookie):\n",
    "    h = hashlib.blake2b(digest_size=AUTH_SIZE, key=secret_key)\n",
    "    h.update(cookie.encode('utf-8'))\n",
    "    return h.hexdigest().encode('utf-8')\n",
    "\n",
    "\n",
    "def verify(cookie, sig):\n",
    "    good_sig = sign(cookie)\n",
    "    return compare_digest(good_sig, sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cookie = {'user': 'alice', 'auth_mode': 'token'}\n",
    "cookie_str = json.dumps(cookie, sort_keys=True)\n",
    "sig = sign(cookie_str)\n",
    "\n",
    "print(cookie_str)\n",
    "print(sig)\n",
    "\n",
    "print(verify(cookie_str, sig))\n",
    "\n",
    "invalid_cookie = {'user': 'admin', 'auth_mode': 'token'}\n",
    "invalid_cookie_str = json.dumps(invalid_cookie, sort_keys=True)\n",
    "print(sign(invalid_cookie_str))\n",
    "\n",
    "print(verify(invalid_cookie_str, sig))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting Duplicate Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "def hash_file(file_path, block_size=1024):\n",
    "    hasher = hashlib.sha256()\n",
    "    with open(file_path, 'rb') as f:\n",
    "        hasher.update(f.read(block_size))\n",
    "    return hasher.hexdigest().encode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generate some random files in a temp directory.\n",
    "temp_dir = tempfile.mkdtemp()\n",
    "\n",
    "print('Temporary Directory: ' + temp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f = open(os.path.join(temp_dir, 'f.txt'), 'wb')\n",
    "f.write(os.urandom(2048))\n",
    "f.close()\n",
    "\n",
    "g = open(os.path.join(temp_dir, 'g.txt'), 'wb')\n",
    "g.write(os.urandom(2048**2))\n",
    "g.close()\n",
    "\n",
    "f_hash = hash_file(os.path.join(temp_dir, 'f.txt'))\n",
    "print(f_hash)\n",
    "\n",
    "g_hash = hash_file(os.path.join(temp_dir, 'g.txt'))\n",
    "print(g_hash)\n",
    "\n",
    "compare_digest(f_hash, g_hash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate random files.\n",
    "FILE_NUM = 1000\n",
    "\n",
    "random_data = (os.urandom(2048) for _ in range(0, FILE_NUM))\n",
    "for indx, random_datum in enumerate(random_data):\n",
    "    with open(os.path.join(temp_dir, '{}.txt'.format(str(indx))), 'wb') as f:\n",
    "        f.write(random_datum)\n",
    "\n",
    "# generate 1 duplicate file\n",
    "random_file = os.path.join(temp_dir, '{}.txt'.format(str(random.randint(0, FILE_NUM - 1))))\n",
    "\n",
    "with open(random_file, 'rb') as f:\n",
    "    with open(os.path.join(temp_dir, 'dup_file.txt'), 'wb') as g:\n",
    "        g.write(f.read())\n",
    "        \n",
    "print(random_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find and list duplicate files.\n",
    "# Algorithm is ~ O(n^2)\n",
    "\n",
    "# Iterate over files in temp_dir\n",
    "for root, directory, files in os.walk(temp_dir):\n",
    "    # pick a file and iterate over it\n",
    "    for o_file in files:\n",
    "        o_fp = os.path.join(root, o_file)\n",
    "        # print('checking: {}'.format(o_fp))\n",
    "        outer_hash = hash_file(o_fp)\n",
    "        for i_file in files:\n",
    "            if o_file != i_file:\n",
    "                i_fp = os.path.join(root, i_file)\n",
    "                inner_hash = hash_file(i_fp)\n",
    "                if compare_digest(outer_hash, inner_hash):\n",
    "                    print('  found duplicate files: {} and {}'.format(o_fp, i_fp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up temp_dir\n",
    "\n",
    "shutil.rmtree(temp_dir)\n",
    "\n",
    "os.path.exists(temp_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
